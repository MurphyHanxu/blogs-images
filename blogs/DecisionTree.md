

+++

author = "Murphy"
title = "决策树--1.决策树的构造"
date = "2022-08-04"

description = ""
tags = [
    "Machine Learning",
]

categories = [
    "CS",
   ]

+++

本章构造的决策树算法能够读取数据集合，构建类似于图1的决策树。

决策树很多任务都是为了数据中所蕴含的知识信息，因此决策树可以使用不熟悉的数据集合，并从中提取出一系列规则，机器学习算法最终将使用这些机器从数据集中创造的规则。

专家系统中经常使用决策树，而且决策树给出结果往往可以匹敌在当前领域具有几十年工作经验的人类专家。

<!--more-->

![DecisionTree](https://raw.githubusercontent.com/MurphyHanxu/blogs-images/master/images/DecisionTree1.png)

## 1.决策树的构造

决策树

优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。

缺点：可能会产生过度匹配问题。

适用数据类型：数值型和标称型。



我们将一步步地构造决策树算法，并会涉及许多有趣的细节。首先我们讨论数学上如何使用信息论划分数据集，然后编写代码将理论应用到具体的数据集上，最后编写代码构造决策树。

在构造决策树时，我们需要解决的第一个问题就是，当前数据集上哪个特征在划分数据分类时起决定性作用。为了找到决定性的特征，划分出最好的结果，我们必须评估每个特征。完成测试之后，原始数据集就被划分为几个数据子集。这些数据子集会分布在第一个决策点的所有分支上。如果某个分支下的数据属于同一类型，则当前无需阅读的垃圾邮件已经正确地划分数据分类，无需进一步对数据集进行分割。如果数据子集内的数据不属于同一类型，则需要重复划分数据子集的过程。如何划分数据子集的算法和划分原始数据集的方法相同，知道所有具有相同类型的数据均在一个数据子集内。



创建分支的伪代码函数createBranch()如下所示：

```
检测数据集中的每个子项是否属于同一分类：
	If so return 类标签:
	Else
		寻找划分数据集的最好特征
		划分数据集
		创建分支节点
			for 每个划分的子集
				调用函数createBranch并增加返回结果到分支节点中
		return 分支节点
```

上面的伪代码createBranch是一个递归函数，在倒数第二行直接调用了它自己。后面将把上述的伪代码转换为Python代码，这里我们需要进一步了解算法是如何划分数据集的。



决策树的一般流程

1.收集数据：可以使用任何方法

2.准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。

3.分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。

4.训练数据：构造树的数据结构。

5.测试算法：使用经验树计算错误率。

6.使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。



下表包含5个海洋动物，特征包括：不浮出水面是否可以生存，以及是否有脚蹼。我们可以将这些动物分成两类：鱼类和非鱼类。现在我们想要决定依据第一个特征还是第二个特征划分数据。在回答这个问题之前，我们必须采用量化的方法判断如何划分数据。

| 编号 | 不浮出水面是否可以生存 | 是否有脚蹼 | 属于鱼类 |
| ---- | ---------------------- | ---------- | -------- |
| 1    | 是                     | 是         | 是       |
| 2    | 是                     | 是         | 是       |
| 3    | 是                     | 否         | 否       |
| 4    | 否                     | 是         | 否       |
| 5    | 否                     | 是         | 否       |



### 1.1信息增益

划分数据集的大原则是：将无序的数据变得更加有序。我们可以使用多种方法划分数据集，但是每种方法都有各自的优缺点。组织杂乱无章数据的一种方法就是使用信息论度量信息，信息论是量化处理信息的分支科学。

我们可以在划分数据之前使用信息论量化度量信息的内容。

在划分数据集之前之后信息发生的变化称为信息增益（information gain），知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。

在可以评测哪种数据划分方式是最好的数据划分之前，我们必须学习如何计算信息增益。集合信息的度量方式称为香农熵或者简称为熵（entropy），这个名字来源于信息论之父克劳德香农。

熵定义为信息的期望值，在明晰这个概念之前，我们必须知道信息的定义。如果待分类的事务可能划分在多个分类之中，则符号$$x_i$$的信息定义为
$$
l(x_i)=-\log_2 p(x_i)
$$
其中$$p(x_i)$$是选择该分类的概率。

为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值，通过下面的公式得到：
$$
H=-\sum_{i=1}^np(x_i)\log_2p(x_i)
$$
其中$$n$$是分类的数目。

下面我们将使用Python计算信息熵，创建名为trees.py的文件，此代码的功能是计算给定数据集的熵。

```python
from math import log
def calcShannonEnt(dataSet):
    numEntries = len(dataSet)
    labelCounts = {}
    #  ①为所有可能分类创建字典
    for featVec in dataSet:
        currentLabel = featVec[-1]
        if currentLabel not in labelCounts.keys():
            labelCounts[currentLabel] = 0
            labelCounts[currentLabel] += 1
    shannonEnt = 0
    #  ②以2为底求对数
    for key in labelCounts:
        prob = float(labelCounts[key])/numEntries
        shannonEnt -= prob * log(prob, 2)
    return shannonEnt
```

首先，计算数据集中实例的总数。我们也可以在需要时再计算这个值，但是由于代码中多次用到这个值，为了提高代码效率，我们显式地声明一个变量保存实例总数。然后，创建一个数据字典，它的键值是最后一列的数值①。如果当前键值不存在，则扩展字典并将当前键值加入字典。每个键值都记录了当前类别出现的次数。最后，使用所有类标签的发生频率计算类别分别出现的概率。我们将用这个概率计算香农熵②，统计所有类标签发生的次数。

下面我们看看如何使用熵划分数据集。

在trees.py中，我们利用creaDataSet()函数得到表的简单数据集。

```python
def createDataSet():
    dataSet = [[1, 1, "yes"],
               [1, 1, "yes"],
               [1, 0, "no"],
               [0, 1, "no"],
               [0, 1, "no"]]
    labels = ["no surfacing", "flippers"]
    return dataSet, labels
```



在Python命令提示符下输入下列命令：

```python
>>>reload(trees.py)
myDat, label = trees.createDataSet()
>>>myDat
[[1, 1, 'maybe'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]
>>>trees.calcShannonEnt(myDat)
0.9287712379549449
```

熵越高，则混合的数据也越多，我们可以在数据集中添加更多的分类，观察熵是如何变化的。这里我们增加第三个名为maybe的分类，测试熵的变化：

```python
>>>myDat[0][-1] = "maybe"
>>>myDat
[[1, 1, 'maybe'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]
>>>trees.calcShannonEnt(myDat)
1.3931568569324173
```

得到熵之后，我们就可以按照获取最大信息增益的方法划分数据集，下一节我们讨论如何划分数据集并创建决策树，以及如何度量信息增益。



### 1.2划分数据集

